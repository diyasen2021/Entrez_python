# How to Automate Fetching PubMed Articles with Python 

If you've ever wanted to retrieve research articles from PubMed in an automated way, this Python script will do just that. We'll be using the Entrez module from Biopython, for fetching articles from PubMed. Here’s how the code works, with each section broken down to help you understand the workflow.

## 1. Environment set up

The script imports necessary libraries, including the Biopython Entrez module for fetching data from NCBI, pandas for data handling, and urllib.error for managing HTTP-related exceptions.

For using NCBI’s Entrez API, it’s necessary to include an email address. Replace 'youremail@example.com' with your actual email address. 

```
import time
from Bio import Entrez
import pandas as pd
import urllib.error  # For handling HTTP errors
Entrez.email = "youremail@example.com"
```

## 2. Define the search function for initial article search

The esearch function is used to perform an initial PubMed search based on a specific query term (e.g., "Streptococcus suis").
This function retrieves all unique PubMed IDs related to the query in a single request.

```
def search(query, retmax=10000):
    """Fetch all IDs matching the query in a single search."""
    handle = Entrez.esearch(db='pubmed',
                            sort='relevance',
                            retmax=retmax,  # Fetch maximum number of results
                            retmode='xml',
                            term=query)
    results = Entrez.read(handle)
    return results
```

## 4. Defining the fetch function to retrieve article details

The fetch_details function accepts a list of article IDs (generated by the search function above) as input and uses the efetch method to retrieve their full details. 

```
def fetch_details(id_list):
    """Fetch details of articles using a list of PubMed IDs."""
    ids = ','.join(id_list)
    handle = Entrez.efetch(db='pubmed',
                           retmode='xml',
                           id=ids)
    results = Entrez.read(handle)
    return results
```

## 5. Fetching Titles and Abstracts 

This piece of the code fetches article details in each batch and extracts the titles and abstracts from each result. If a title or abstract is missing, it appends "NA" as a placeholder.

```
        try:
            papers = fetch_details(batch_ids)
            for paper in papers['PubmedArticle']:
                try:
                    title = paper['MedlineCitation']['Article']['ArticleTitle']
                    title_list.append(title)
                except:
                    title_list.append("NA")
                try:
                    abstract = paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0]
                    abstract_list.append(abstract)
                except:
                    abstract_list.append("NA")
```

## 7. Error handling 

The code handles error at multiple levels. 

## 7. Executing the Main Code Block

The main code block initiates by setting a specific search term (e.g., "Streptococcus suis") to query PubMed for relevant articles. This query is passed to the esearch function, which retrieves all matching PubMed IDs for that term.

The list of PubMed IDs from esearch is split into batches, determined by a preset batch size (e.g., 50 IDs per batch). For each batch, the script runs the efetch function to retrieve article details (titles and abstracts).

This batch approach ensures efficient data retrieval while respecting PubMed's usage limitations, enabling the processing of large datasets in a structured and manageable way.

The final output is printed to a csv file. 

Putting it all together;


```
import time
from Bio import Entrez
import pandas as pd
import urllib.error

# Provide your email address to the Entrez system
Entrez.email = "youremail@example.com"

def esearch(query, retmax=10000):
    """Fetch all IDs matching the query in a single search."""
    handle = Entrez.esearch(db='pubmed',
                            sort='relevance',
                            retmax=retmax,  # Fetch maximum number of results
                            retmode='xml',
                            term=query)
    results = Entrez.read(handle)
    return results

def efetch(id_list):
    """Fetch details of articles using a list of PubMed IDs."""
    print(f"Fetching {len(id_list)} IDs")
    ids = ','.join(id_list)
    handle = Entrez.efetch(db='pubmed',
                           retmode='xml',
                           id=ids)
    results = Entrez.read(handle)
    return results

# Main code
title_list = []
abstract_list = []
batch_size = 50  # Fetch articles in batches of XX
sleep_time = 10  # Sleep for 10 seconds between requests
max_retries = 5  # Maximum number of retries for a failed batch

if __name__ == '__main__':
    term = "Streptococcus suis"

    # Single search to get all PubMed IDs for the query
    initial_results = esearch(term)
    total_count = int(initial_results['Count'])
    id_list = initial_results['IdList']
    print(f"Total number of articles: {total_count}")

    # Process the list of IDs in batches using efetch
    for start in range(0, len(id_list), batch_size):
        batch_ids = id_list[start:start+batch_size]  # Get a chunk of IDs

        retries = 0
        while retries < max_retries:
            try:
                # Sleep before making the efetch request to avoid rate limits
                time.sleep(sleep_time)

                # Fetch article details for the current batch
                papers = efetch(batch_ids)

                # Process the fetched papers
                for i, paper in enumerate(papers['PubmedArticle']):
                    try:
                        title = paper['MedlineCitation']['Article']['ArticleTitle']
                        title_list.append(title)
                    except:
                        title_list.append("NA")
                    try:
                        abstract = paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0]
                        abstract_list.append(abstract)
                    except:
                        abstract_list.append("NA")

                # Break out of the retry loop if the batch is successful
                break

            except urllib.error.HTTPError as e:
                # Handle HTTP error and retry the batch
                retries += 1
                print(f"HTTP error occurred: {e}, retrying... ({retries}/{max_retries})")
                if retries == max_retries:
                    print(f"Max retries reached for batch {batch_ids}. Skipping this batch.")

    # Create a DataFrame from the collected titles and abstracts
    data = list(zip(title_list, abstract_list))
    df = pd.DataFrame(data, columns=['Title', 'Abstract'])
    df.to_csv('pubmed_articles.csv', index=False)

    # Print the DataFrame to check the result
    print(df)
```
